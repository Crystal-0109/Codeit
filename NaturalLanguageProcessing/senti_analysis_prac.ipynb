{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parkf\\AppData\\Local\\Temp\\ipykernel_15068\\1518571348.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "C:\\Users\\parkf\\AppData\\Local\\Temp\\ipykernel_15068\\1518571348.py:27: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from preprocess import clean_by_freq\n",
    "from preprocess import clean_by_len\n",
    "from preprocess import clean_by_stopwords\n",
    "from preprocess import stemming_by_porter\n",
    "from preprocess import penn_to_wn\n",
    "from preprocess import pos_tagger\n",
    "from preprocess import words_lemmatizer\n",
    "from preprocess import swn_polarity\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 데이터 불러오기\n",
    "df = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")\n",
    "\n",
    "# 대소문자 통합\n",
    "df['review'] = df['review'].str.lower()\n",
    "\n",
    "# 문장 토큰화\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)\n",
    "\n",
    "# 품사 태깅\n",
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n",
    "\n",
    "# 표제어 추출\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "\n",
    "# 추가 전처리\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(``, ``), (watching, JJ), (time, NN), (chaser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(``, ``), (yes, RB), (,, ,), (i, JJ), (agree,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(``, ``), (jennifer, NN), (ehle, NN), (was, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[(``, ``), (a, DT), (plane, NN), (carrying, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[(``, ``), (incredibly, RB), (dumb, JJ), (and,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   pos_tagged_tokens\n",
       "0  [(``, ``), (watching, JJ), (time, NN), (chaser...\n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...\n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...\n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...\n",
       "4  [(``, ``), (yes, RB), (,, ,), (i, JJ), (agree,...\n",
       "5  [(``, ``), (jennifer, NN), (ehle, NN), (was, V...\n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...\n",
       "7  [(``, ``), (a, DT), (plane, NN), (carrying, VB...\n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...\n",
       "9  [(``, ``), (incredibly, RB), (dumb, JJ), (and,..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['pos_tagged_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentiWordnet 감성 분석\n",
    "pos_tagged_words = df['pos_tagged_tokens'][0]\n",
    "senti_score = 0  # 코퍼스 안의 단어들을 하나씩 순회하며 계산한 각 단어 감성 지수의 총합을 저장하는 변수\n",
    "\n",
    "for word, tag in pos_tagged_words:\n",
    "    # PennTreebank Tag로 태깅된 품사를 WordNet Tag 기준으로 변경\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    # WordNet Tag에 포함되지 않는 경우는 제외\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "        continue\n",
    "    \n",
    "    # Synset 확인, 어휘 사전에 없을 경우에는 제외\n",
    "    if not wn.synsets(word, wn_tag):\n",
    "        continue\n",
    "    else:\n",
    "        synsets = wn.synsets(word, wn_tag)\n",
    "    \n",
    "    # SentiSynset 확인\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    \n",
    "    # 감성 지수 계산\n",
    "    word_senti_score = (swn_synset.pos_score() - swn_synset.neg_score())\n",
    "    senti_score += word_senti_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.375\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(senti_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>swn_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"watching time chasers, it obvious that it was...</td>\n",
       "      <td>-0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>-1.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>-2.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"yes, i agree with everyone on this site this ...</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"jennifer ehle was sparkling in \\\"\"pride and p...</td>\n",
       "      <td>6.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"a plane carrying employees of a large biotech...</td>\n",
       "      <td>8.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>4.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"incredibly dumb and utterly predictable story...</td>\n",
       "      <td>-1.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  swn_sentiment\n",
       "0  \"watching time chasers, it obvious that it was...         -0.375\n",
       "1  i saw this film about 20 years ago and remembe...         -1.500\n",
       "2  minor spoilers in new york, joan barnard (elvi...         -2.250\n",
       "3  i went to see this film with a great deal of e...         -0.500\n",
       "4  \"yes, i agree with everyone on this site this ...          3.000\n",
       "5  \"jennifer ehle was sparkling in \\\"\"pride and p...          6.750\n",
       "6  amy poehler is a terrific comedian on saturday...          0.750\n",
       "7  \"a plane carrying employees of a large biotech...          8.750\n",
       "8  a well made, gritty science fiction movie, it ...          4.500\n",
       "9  \"incredibly dumb and utterly predictable story...         -1.125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe에 swn_polarity() 함수 적용\n",
    "df['swn_sentiment'] = df['pos_tagged_tokens'].apply(swn_polarity)\n",
    "\n",
    "df[['review', 'swn_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"watching time chasers, it obvious that it was made by a bunch of friends. maybe they were sitting around one day in film school and said, \\\\\"\"hey, let\\'s pool our money together and make a really bad movie!\\\\\"\" or something like that. what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. all corners were cut, except the one that would have prevented this film\\'s release. life\\'s like that.\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i saw this film about 20 years ago and remember it as being particularly nasty. i believe it is based on a true incident: a young man breaks into a nurses' home and rapes, tortures and kills various women. it is in black and white but saves the colour for one shocking shot. at the end the film seems to be trying to make some political statement but it just comes across as confused and obscene. avoid.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a well made, gritty science fiction movie, it could be lost among hundreds of other similar movies, but it has several strong points to keep it near the top. for one, the writing and directing is very solid, and it manages for the most part to avoid many sci-fi cliches, though not all of them. it does a good job of keeping you in suspense, and the landscape and look of the movie will appeal to sci-fi fans. if you're looking for a masterpiece, this isn't it. but if you're looking for good old fashioned post-apoc, gritty future in space sci-fi, with good suspense and special effects, then this is the movie for you. thoroughly enjoyable, and a good ending.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
