{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parkf\\AppData\\Local\\Temp\\ipykernel_19668\\2903358726.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\parkf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\parkf\\AppData\\Local\\Temp\\ipykernel_19668\\2903358726.py:20: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[35, 36, 78, 78, 35, 36, 79, 79, 35, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[123, 123]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...\n",
       "1                                             [2, 2]\n",
       "2  [55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...\n",
       "3  [2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...\n",
       "4  [72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...\n",
       "5           [35, 36, 78, 78, 35, 36, 79, 79, 35, 36]\n",
       "6  [80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...\n",
       "7  [85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...\n",
       "8  [120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...\n",
       "9                                         [123, 123]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from preprocess import pos_tagger\n",
    "from preprocess import words_lemmatizer\n",
    "from preprocess import clean_by_freq, clean_by_len, clean_by_stopwords\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('imdb.tsv', delimiter = \"\\\\t\")\n",
    "\n",
    "# 소문자로 정규화\n",
    "df['review'] = df['review'].str.lower()\n",
    "\n",
    "# 문장 토큰화\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)\n",
    "\n",
    "# 품사 태깅\n",
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n",
    "\n",
    "# 표제어 추출\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "\n",
    "# 추가 전처리 (빈도 1 이하, 단어 길이 2 이하 단어 제거, 불용어 제거)\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x: clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x: clean_by_stopwords(x, stopwords_set))\n",
    "\n",
    "df[['cleaned_tokens']]\n",
    "\n",
    "# 정수 인코딩\n",
    "tokens = df['cleaned_tokens'][4]\n",
    "\n",
    "vocab = Counter(tokens)\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "# 각 단어에 인덱스 부여\n",
    "word_to_idx = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "# 토큰들을 부여된 인덱스로 변경\n",
    "encoded_idx = []\n",
    "\n",
    "for token in tokens:\n",
    "    idx = word_to_idx[token]\n",
    "    encoded_idx.append(idx)\n",
    "\n",
    "# 전체 코퍼스의 토큰들을 전부 합하여 단어의 등장 빈도를 계산\n",
    "tokens = sum(df['cleaned_tokens'], [])\n",
    "\n",
    "# 합쳐진 토큰 리스트로 빈도 계산, 많이 등장한 순으로 정렬하여 정수 인덱스 부여\n",
    "word_to_idx = {}\n",
    "i = 0\n",
    "tokens = sum(df['cleaned_tokens'], [])\n",
    "\n",
    "vocab = Counter(tokens)\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_idx[word] = i\n",
    "\n",
    "# 데이터 프레임의 각 로우에 있는 토큰들을 정수 인코딩\n",
    "def idx_encoder(tokens, word_to_idx):\n",
    "    encoded_idx = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        idx = word_to_idx[token]\n",
    "        encoded_idx.append(idx)\n",
    "        \n",
    "    return encoded_idx\n",
    "\n",
    "df['integer_encoded'] = df['cleaned_tokens'].apply(lambda x: idx_encoder(x, word_to_idx))\n",
    "df[['integer_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 최대 개수: 200\n"
     ]
    }
   ],
   "source": [
    "# 제로 패딩\n",
    "max_len = max(len(item) for item in df['integer_encoded'])\n",
    "\n",
    "print('토큰의 최대 개수:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[35, 36, 78, 78, 35, 36, 79, 79, 35, 36, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[123, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     integer_encoded\n",
       "0  [8, 11, 2, 54, 8, 16, 5, 1, 12, 54, 8, 16, 5, ...\n",
       "1  [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2  [55, 56, 19, 57, 58, 59, 57, 60, 61, 25, 62, 6...\n",
       "3  [2, 2, 30, 69, 3, 30, 31, 3, 69, 70, 71, 30, 7...\n",
       "4  [72, 1, 5, 17, 1, 1, 8, 1, 73, 74, 1, 1, 74, 1...\n",
       "5  [35, 36, 78, 78, 35, 36, 79, 79, 35, 36, 0, 0,...\n",
       "6  [80, 1, 3, 21, 81, 82, 21, 21, 80, 3, 82, 83, ...\n",
       "7  [85, 86, 37, 38, 87, 39, 86, 88, 40, 89, 41, 9...\n",
       "8  [120, 1, 1, 121, 52, 10, 121, 122, 53, 1, 52, ...\n",
       "9  [123, 123, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다른 코퍼스들의 길이가 200이 되도록 0 채워넣기\n",
    "for tokens in df['integer_encoded']:\n",
    "    while len(tokens) < max_len:\n",
    "        tokens.append(0)\n",
    "\n",
    "df[['integer_encoded']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
